total <- merge( mae0s, mae1s,by=c("Group.1","Group.2"))
mae0  <- unlist(c(total["mae0"]), use.names=FALSE) #7 ad4_ic
mae1  <- unlist(c(total["mae1"]), use.names=FALSE) #7 ad4_ic
relMAE <- mae1/mae0 # base 'relative' to own2, mae0 is for own2
#AvgRelMAE
AvgRelMAE2<- prod(relMAE)^(1/length(relMAE))
#4, ow2_ew relative to own2
data0 <- overall_r_own2_versus_r_ow2_ew
mae0 <- aggregate(data0$ae_0, by=list(data0$category, data0$sku), mean)
mae1 <- aggregate(data0$ae_1, by=list(data0$category, data0$sku), mean)
mae0s <- mae0[order(mae0$Group.1, mae0$Group.2),]
colnames(mae0s)[3] <- "mae0"
mae1s <- mae1[order(mae1$Group.1, mae1$Group.2),]
colnames(mae1s)[3] <- "mae1"
total <- merge( mae0s, mae1s,by=c("Group.1","Group.2"))
mae0  <- unlist(c(total["mae0"]), use.names=FALSE) #7 ad4_ic
mae1  <- unlist(c(total["mae1"]), use.names=FALSE) #7 ad4_ic
relMAE <- mae1/mae0 # base 'relative' to own2, mae0 is for own2
#AvgRelMAE
AvgRelMAE3<- prod(relMAE)^(1/length(relMAE))
#5, ow2_ic relative to own2
data0 <- overall_r_own2_versus_r_ow2_ic
mae0 <- aggregate(data0$ae_0, by=list(data0$category, data0$sku), mean)
mae1 <- aggregate(data0$ae_1, by=list(data0$category, data0$sku), mean)
mae0s <- mae0[order(mae0$Group.1, mae0$Group.2),]
colnames(mae0s)[3] <- "mae0"
mae1s <- mae1[order(mae1$Group.1, mae1$Group.2),]
colnames(mae1s)[3] <- "mae1"
total <- merge( mae0s, mae1s,by=c("Group.1","Group.2"))
mae0  <- unlist(c(total["mae0"]), use.names=FALSE) #7 ad4_ic
mae1  <- unlist(c(total["mae1"]), use.names=FALSE) #7 ad4_ic
relMAE <- mae1/mae0 # base 'relative' to own2, mae0 is for own2
#AvgRelMAE
AvgRelMAE4<- prod(relMAE)^(1/length(relMAE))
#6, ad4_ew relative to own2
data0 <- overall_r_own2_versus_r_ow2_ic
data1 <- overall_r_adl4_versus_r_ad4_ew
mae0 <- aggregate(data0$ae_0, by=list(data0$category, data0$sku), mean)
mae1 <- aggregate(data1$ae_1, by=list(data1$category, data0$sku), mean)
mae0s <- mae0[order(mae0$Group.1, mae0$Group.2),]
colnames(mae0s)[3] <- "mae0"
mae1s <- mae1[order(mae1$Group.1, mae1$Group.2),]
colnames(mae1s)[3] <- "mae1"
total <- merge( mae0s, mae1s,by=c("Group.1","Group.2"))
mae0  <- unlist(c(total["mae0"]), use.names=FALSE) #7 ad4_ic
mae1  <- unlist(c(total["mae1"]), use.names=FALSE) #7 ad4_ic
relMAE <- mae1/mae0 # base 'relative' to own2, mae0 is for own2
#AvgRelMAE
AvgRelMAE5<- prod(relMAE)^(1/length(relMAE))
#7, ad4_ic relative to own2
data0 <- overall_r_own2_versus_r_ow2_ic
data1 <- overall_r_adl4_versus_r_ad4_ic
mae0 <- aggregate(data0$ae_0, by=list(data0$category, data0$sku), mean)
mae1 <- aggregate(data1$ae_1, by=list(data1$category, data0$sku), mean)
mae0s <- mae0[order(mae0$Group.1, mae0$Group.2),]
colnames(mae0s)[3] <- "mae0"
mae1s <- mae1[order(mae1$Group.1, mae1$Group.2),]
colnames(mae1s)[3] <- "mae1"
total <- merge( mae0s, mae1s,by=c("Group.1","Group.2"))
mae0  <- unlist(c(total["mae0"]), use.names=FALSE) #7 ad4_ic
mae1  <- unlist(c(total["mae1"]), use.names=FALSE) #7 ad4_ic
relMAE <- mae1/mae0 # base 'relative' to own2, mae0 is for own2
#AvgRelMAE
AvgRelMAE6<- prod(relMAE)^(1/length(relMAE))
#8, ewc_ic relative to own2
data0 <- overall_r_own2_versus_r_ow2_ic
data1 <- overall_r_adl4_versus_r_ewc_ic
mae0 <- aggregate(data0$ae_0, by=list(data0$category, data0$sku), mean)
mae1 <- aggregate(data1$ae_1, by=list(data1$category, data0$sku), mean)
mae0s <- mae0[order(mae0$Group.1, mae0$Group.2),]
colnames(mae0s)[3] <- "mae0"
mae1s <- mae1[order(mae1$Group.1, mae1$Group.2),]
colnames(mae1s)[3] <- "mae1"
total <- merge( mae0s, mae1s,by=c("Group.1","Group.2"))
mae0  <- unlist(c(total["mae0"]), use.names=FALSE) #7 ad4_ic
mae1  <- unlist(c(total["mae1"]), use.names=FALSE) #7 ad4_ic
relMAE <- mae1/mae0 # base 'relative' to own2, mae0 is for own2
#AvgRelMAE
AvgRelMAE7<- prod(relMAE)^(1/length(relMAE))
#### organize
own2 <- 1
results_vector <- rbind(
AvgRelMAE1,
own2,
AvgRelMAE2,
AvgRelMAE3,
AvgRelMAE4,
AvgRelMAE5,
AvgRelMAE6,
AvgRelMAE7)
rank <- match(results_vector, sort(results_vector))#the order of the perforamnce of the models
results_rank_AvgRelMAE <- cbind(results_vector, rank) #combine the model results with the ranks
#### all results ;
all_results_ranks  <- cbind(results_rank_mae, results_rank_smape, results_rank_mase, results_rank_AvgRelMAE, results_rank_mape, results_rank_scaled_mse)
all_results_pvalue <- cbind(pvalue_mae, pvalue_smape, pvalue_mase, pvalue_mape, pvalue_scaled_mse)
library(forecast)
#library(tstools)
library(zoo)
library(PMCMR)
library(NSM3)
rm(list=ls())
setwd("Q:/= IRI data research/Manuscript for  SUBMISSION/= rank results and conduct significant test/Results and Diebold and Mariana test/results data")
#dm.test(e1,e2,h=horizon_parm2,power=1)
#dm test input 'e1' and 'e2' could be the original error, the function will automatically take the absolute value
overall_r_adl4_versus_r_ad4_ew <- read.csv("overall_r_adl4_versus_r_ad4_ew.csv")
overall_r_adl4_versus_r_ad4_ic <- read.csv("overall_r_adl4_versus_r_ad4_ic.csv")
overall_r_adl4_versus_r_ewc_ic <- read.csv("overall_r_adl4_versus_r_ewc_ic.csv")
overall_r_own2_versus_r_adl4   <- read.csv("overall_r_own2_versus_r_adl4.csv")
overall_r_own2_versus_r_base   <- read.csv("overall_r_own2_versus_r_base.csv")
overall_r_own2_versus_r_ow2_ew <- read.csv("overall_r_own2_versus_r_ow2_ew.csv")
overall_r_own2_versus_r_ow2_ic <- read.csv("overall_r_own2_versus_r_ow2_ic.csv")
#remove the obs beyond the horizon
horizon_parm <- 8
overall_r_adl4_versus_r_ad4_ew <- overall_r_adl4_versus_r_ad4_ew[overall_r_adl4_versus_r_ad4_ew$horizon <=horizon_parm,]
overall_r_adl4_versus_r_ad4_ic <- overall_r_adl4_versus_r_ad4_ic[overall_r_adl4_versus_r_ad4_ic$horizon <=horizon_parm,]
overall_r_adl4_versus_r_ewc_ic <- overall_r_adl4_versus_r_ewc_ic[overall_r_adl4_versus_r_ewc_ic$horizon <=horizon_parm,]
overall_r_own2_versus_r_adl4   <- overall_r_own2_versus_r_adl4[overall_r_own2_versus_r_adl4$horizon <=horizon_parm,]
overall_r_own2_versus_r_base   <- overall_r_own2_versus_r_base[overall_r_own2_versus_r_base$horizon <=horizon_parm,]
overall_r_own2_versus_r_ow2_ew <- overall_r_own2_versus_r_ow2_ew[overall_r_own2_versus_r_ow2_ew$horizon <=horizon_parm,]
overall_r_own2_versus_r_ow2_ic <- overall_r_own2_versus_r_ow2_ic[overall_r_own2_versus_r_ow2_ic$horizon <=horizon_parm,]
#3. test for MAPE
f1  <- unlist(c(overall_r_own2_versus_r_base["forecast_sb_adjusted"]), use.names=FALSE)   #1 base
f2  <- unlist(c(overall_r_own2_versus_r_base["forecast_original"]), use.names=FALSE)      #2 own2
f3  <- unlist(c(overall_r_own2_versus_r_adl4["forecast_sb_adjusted"]), use.names=FALSE)   #3 adl4
f4  <- unlist(c(overall_r_own2_versus_r_ow2_ew["forecast_sb_adjusted"]), use.names=FALSE) #4 ow2_ew
f5  <- unlist(c(overall_r_own2_versus_r_ow2_ic["forecast_sb_adjusted"]), use.names=FALSE) #5 ow2_ic
f6  <- unlist(c(overall_r_adl4_versus_r_ad4_ew["forecast_sb_adjusted"]), use.names=FALSE) #6 ad4_ew
f7  <- unlist(c(overall_r_adl4_versus_r_ad4_ic["forecast_sb_adjusted"]), use.names=FALSE) #7 ad4_ic
f8  <- unlist(c(overall_r_adl4_versus_r_ewc_ic["forecast_sb_adjusted"]), use.names=FALSE) #8 ewc_ic
actual<- unlist(c(overall_r_adl4_versus_r_ewc_ic["actual"]), use.names=FALSE) #actual
pe1 <- (f1-actual)/actual
pe2 <- (f2-actual)/actual
pe3 <- (f3-actual)/actual
pe4 <- (f4-actual)/actual
pe5 <- (f5-actual)/actual
pe6 <- (f6-actual)/actual
pe7 <- (f7-actual)/actual
pe8 <- (f8-actual)/actual
mean(pe1)
mean(pe2)
mean(pe3)
mean(pe4)
mean(pe5)
mean(pe6)
mean(pe7)
mean(pe8)
results_vector <- rbind(
mean(pe1),
mean(pe2),
mean(pe3),
mean(pe4),
mean(pe5),
mean(pe6),
mean(pe7),
mean(pe8))
rm(list=ls())
setwd("Q:/= IRI data research/Manuscript for  SUBMISSION/= rank results and conduct significant test/Results and Diebold and Mariana test/results data")
test_cuts_vs_splines <- function(signal, N, noise,
range=c(0, 1),
max_parameters=50,
seed=154) {
if(max_parameters < 8) {
stop("Please pass max_parameters >= 8, otherwise the plots look kinda bad.")
}
out_obj <- list()
set.seed(seed)
x_train <- runif(N, range[1], range[2])
x_test <- runif(N, range[1], range[2])
y_train <- signal(x_train) + rnorm(N, 0, noise)
y_test <- signal(x_test) + rnorm(N, 0, noise)
df <- data.frame(
x = seq(range[1], range[2], length.out = 100)
)
df$y <- signal(df$x)
out_obj$signal_plot <- ggplot(data = df) +
geom_line(aes(x = x, y = y)) +
labs(title = "True Signal")
# A plot of the training and testing data
df <- data.frame(
x = c(x_train, x_test),
y = c(y_train, y_test),
id = c(rep("train", N), rep("test", N))
)
out_obj$data_plot <- ggplot(data = df) +
geom_point(aes(x=x, y=y)) +
facet_wrap(~ id) +
labs(title = "Training and Testing Data")
#----- lm with various groupings -------------
models_with_groupings <- list()
train_errors_cuts <- rep(NULL, length(models_with_groupings))
test_errors_cuts <- rep(NULL, length(models_with_groupings))
for (n_groups in 3:max_parameters) {
cut_points <- seq(range[1], range[2], length.out = n_groups + 1)
x_train_factor <- cut(x_train, cut_points)
factor_train_data <- data.frame(x = x_train_factor, y = y_train)
models_with_groupings[[n_groups]] <- lm(y ~ x, data = factor_train_data)
# Training error rate
train_preds <- predict(models_with_groupings[[n_groups]], factor_train_data)
soses <- (1/N) * sum( (y_train - train_preds)**2)
train_errors_cuts[n_groups - 2] <- soses
# Testing error rate
x_test_factor <- cut(x_test, cut_points)
factor_test_data <- data.frame(x = x_test_factor, y = y_test)
test_preds <- predict(models_with_groupings[[n_groups]], factor_test_data)
soses <- (1/N) * sum( (y_test - test_preds)**2)
test_errors_cuts[n_groups - 2] <- soses
}
# We are overfitting
error_df_cuts <- data.frame(
x = rep(3:max_parameters, 2),
e = c(train_errors_cuts, test_errors_cuts),
id = c(rep("train", length(train_errors_cuts)),
rep("test", length(test_errors_cuts))),
type = "cuts"
)
out_obj$errors_cuts_plot <- ggplot(data = error_df_cuts) +
geom_line(aes(x = x, y = e)) +
facet_wrap(~ id) +
labs(title = "Error Rates with Grouping Transformations",
x = ("Number of Estimated Parameters"),
y = ("Average Squared Error"))
#----- lm with natural splines -------------
models_with_splines <- list()
train_errors_splines <- rep(NULL, length(models_with_groupings))
test_errors_splines <- rep(NULL, length(models_with_groupings))
for (deg_freedom in 3:max_parameters) {
knots <- seq(range[1], range[2], length.out = deg_freedom + 1)[2:deg_freedom]
train_data <- data.frame(x = x_train, y = y_train)
models_with_splines[[deg_freedom]] <- lm(y ~ ns(x, knots=knots), data = train_data)
# Training error rate
train_preds <- predict(models_with_splines[[deg_freedom]], train_data)
soses <- (1/N) * sum( (y_train - train_preds)**2)
train_errors_splines[deg_freedom - 2] <- soses
# Testing error rate
test_data <- data.frame(x = x_test, y = y_test)
test_preds <- predict(models_with_splines[[deg_freedom]], test_data)
soses <- (1/N) * sum( (y_test - test_preds)**2)
test_errors_splines[deg_freedom - 2] <- soses
}
error_df_splines <- data.frame(
x = rep(3:max_parameters, 2),
e = c(train_errors_splines, test_errors_splines),
id = c(rep("train", length(train_errors_splines)),
rep("test", length(test_errors_splines))),
type = "splines"
)
out_obj$errors_splines_plot <- ggplot(data = error_df_splines) +
geom_line(aes(x = x, y = e)) +
facet_wrap(~ id) +
labs(title = "Error Rates with Natural Cubic Spline Transformations",
x = ("Number of Estimated Parameters"),
y = ("Average Squared Error"))
error_df <- rbind(error_df_cuts, error_df_splines)
out_obj$error_df <- error_df
# The training error for the first cut model is always an outlier, and
# messes up the y range of the plots.
y_lower_bound <- min(c(train_errors_cuts, train_errors_splines))
y_upper_bound = train_errors_cuts[2]
out_obj$errors_comparison_plot <- ggplot(data = error_df) +
geom_line(aes(x = x, y = e)) +
facet_wrap(~ id*type) +
scale_y_continuous(limits = c(y_lower_bound, y_upper_bound)) +
labs(
title = ("Binning vs. Natural Splines"),
x = ("Number of Estimated Parameters"),
y = ("Average Squared Error"))
out_obj
}
View(test_cuts_vs_splines)
View(test_cuts_vs_splines)
test_cuts_vs_splines <- function(signal, N, noise,
range=c(0, 1),
max_parameters=50,
seed=154)
View(test_cuts_vs_splines)
test_cuts_vs_splines <- function(signal, N, noise,
range=c(0, 1),
max_parameters=50,
seed=154)
test_cuts_vs_splines
true_signal_sin <- function(x) {
x + 1.5*sin(3*2*pi*x)
}
obj <- test_cuts_vs_splines(true_signal_sin, 250, 1)
View(obj)
install.packages("splines2")
library(forecast)
#library(tstools)
library(zoo)
library(PMCMR)
library(NSM3)
rm(list=ls())
setwd("Q:/= IRI data research/Manuscript for  SUBMISSION/= output improvement per category boxplot")
data0 <- read.csv("All_comparison_all_8_mase.csv")
View(data0)
data0
mase_adl4     <- unlist(c(data0["adl4"]), use.names=FALSE)   #1 base
mase_ad4_ew   <- unlist(c(data0["ad4_ew"]), use.names=FALSE)   #1 base
View(data0)
mase_adl4
View(data0)
#1. test for MAE
mase_adl4     <- unlist(c(data0["adl4"]), use.names=FALSE)
mase_ad4_ew   <- unlist(c(data0["ad4_ew"]), use.names=FALSE)
mase_ad4_ic   <- unlist(c(data0["ad4_ic"]), use.names=FALSE)
pct_improvement_adl_ew <- mase_ad4_ew/mase_adl4
pct_improvement_adl_ic <- mase_ad4_ic/mase_adl4
pct_improvement_adl_ic
data1 <- cbind(category, sku,pct_improvement_adl_ew, pct_improvement_adl_ic)
#1. test for MAE
mase_adl4     <- unlist(c(data0["adl4"]), use.names=FALSE)
mase_ad4_ew   <- unlist(c(data0["ad4_ew"]), use.names=FALSE)
mase_ad4_ic   <- unlist(c(data0["ad4_ic"]), use.names=FALSE)
category   <- unlist(c(data0["category"]), use.names=FALSE)
sku   <- unlist(c(data0["sku"]), use.names=FALSE)
pct_improvement_adl_ew <- mase_ad4_ew/mase_adl4
pct_improvement_adl_ic <- mase_ad4_ic/mase_adl4
data1 <- cbind(category, sku,pct_improvement_adl_ew, pct_improvement_adl_ic)
View(data1)
boxplot(data1$cateogry~data1$pct_improvement_adl_ew)
data1$cateogry
class(data1)
data1_dataframe<- data.frame(data1)
boxplot(data1_dataframe$cateogry~data1_dataframe$pct_improvement_adl_ew)
data1_dataframe<- data.frame(data1)
data1_dataframe
data1_dataframe$cateogry
data1_dataframe$"cateogry"
data1_dataframe["cateogry"]
data1["cateogry"]
class(data0)
data1_dataframe<- data.frame(data1)
class(data1_dataframe)
data1_dataframe<- data.frame(data1)
boxplot(data1_dataframe)
boxplot(data1_dataframe$pct_improvement_adl_ew)
boxplot(data1_dataframe$category~data1_dataframe$pct_improvement_adl_ic)
boxplot(data1_dataframe$pct_improvement_adl_ic~data1_dataframe$category)
pct_improvement_adl_ew <- (mase_adl4-mase_ad4_ew)/mase_adl4
pct_improvement_adl_ic <- (mase_adl4-mase_ad4_ic)/mase_adl4
data1 <- cbind(category, sku,pct_improvement_adl_ew, pct_improvement_adl_ic)
data1_dataframe<- data.frame(data1)
class(data1_dataframe)
boxplot(data1_dataframe$pct_improvement_adl_ic~data1_dataframe$category)
data1_dataframe
View(data0)
category
k <- data1_dataframe[data1_dataframe[,3] == "beer",]
k
k <- data1_dataframe[data1_dataframe[,3] == "beer"]
k
k <- data1_dataframe[data1_dataframe[,3] == ,"beer"]
install.packages("dplyr")
library(dplyr)
result <- filter(data1_dataframe, category == "beer")
library(dplyr)
install.packages("dplyr")
library(dplyr)
result <- filter(data1_dataframe, category == "beer")
result
library(dplyr)
result <- filter(data1_dataframe, category == beer)
result <- filter(data1_dataframe, category == "beer")
result
result <- filter(data1_dataframe[,3] == "beer")
result <-  subset(data1_dataframe, category %in% c("beer", "fzpizza" ))
result
data1_dataframe
result <-  subset(data0, category %in% c("beer", "fzpizza" ))
View(result)
boxplot(result$pct_improvement_adl_ic~result$category)
class(data1_dataframe)
View(result)
class(data0)
result <-  subset(data0, category %in% c("beer", "fzpizza" ))
#1. test for MAE
mase_adl4     <- unlist(c(result["adl4"]), use.names=FALSE)
mase_ad4_ew   <- unlist(c(result["ad4_ew"]), use.names=FALSE)
mase_ad4_ic   <- unlist(c(result["ad4_ic"]), use.names=FALSE)
category   <- unlist(c(result["category"]), use.names=FALSE)
sku   <- unlist(c(result["sku"]), use.names=FALSE)
pct_improvement_adl_ew <- (mase_adl4-mase_ad4_ew)/mase_adl4
pct_improvement_adl_ic <- (mase_adl4-mase_ad4_ic)/mase_adl4
data1 <- cbind(category, sku,pct_improvement_adl_ew, pct_improvement_adl_ic)
data1_dataframe<- data.frame(data1)
class(data1_dataframe)
boxplot(result$pct_improvement_adl_ic~result$category)
boxplot(data1_dataframe$pct_improvement_adl_ic~data1_dataframe$category)
class(data0)
result <-  subset(data0, category %in% c("factiss", "hhclean", "spagsauc", "toitisu", "toothpa", "yogurt" ))
#1. test for MAE
mase_adl4     <- unlist(c(result["adl4"]), use.names=FALSE)
mase_ad4_ew   <- unlist(c(result["ad4_ew"]), use.names=FALSE)
mase_ad4_ic   <- unlist(c(result["ad4_ic"]), use.names=FALSE)
category   <- unlist(c(result["category"]), use.names=FALSE)
sku   <- unlist(c(result["sku"]), use.names=FALSE)
pct_improvement_adl_ew <- (mase_adl4-mase_ad4_ew)/mase_adl4
pct_improvement_adl_ic <- (mase_adl4-mase_ad4_ic)/mase_adl4
data1 <- cbind(category, sku,pct_improvement_adl_ew, pct_improvement_adl_ic)
data1_dataframe<- data.frame(data1)
class(data1_dataframe)
boxplot(data1_dataframe$pct_improvement_adl_ic~data1_dataframe$category)
View(data0)
boxplot(data1_dataframe$pct_improvement_adl_ew~data1_dataframe$category)
library(forecast)
#library(tstools)
library(zoo)
library(PMCMR)
library(NSM3)
rm(list=ls())
setwd("Q:/= IRI data research/Diebold Mariana Test/results data")
#dm.test(e1,e2,h=horizon_parm2,power=1)
#dm test input 'e1' and 'e2' could be the original error, the function will automatically take the absolute value
overall_r_adl4_versus_r_ad4_ew <- read.csv("overall_r_adl4_versus_r_ad4_ew.csv")
overall_r_adl4_versus_r_ad4_ic <- read.csv("overall_r_adl4_versus_r_ad4_ic.csv")
overall_r_adl4_versus_r_ewc_ic <- read.csv("overall_r_adl4_versus_r_ewc_ic.csv")
overall_r_own2_versus_r_adl4   <- read.csv("overall_r_own2_versus_r_adl4.csv")
overall_r_own2_versus_r_base   <- read.csv("overall_r_own2_versus_r_base.csv")
overall_r_own2_versus_r_ow2_ew <- read.csv("overall_r_own2_versus_r_ow2_ew.csv")
overall_r_own2_versus_r_ow2_ic <- read.csv("overall_r_own2_versus_r_ow2_ic.csv")
#remove the obs beyond the horizon
horizon_parm <- 8
overall_r_adl4_versus_r_ad4_ew <- overall_r_adl4_versus_r_ad4_ew[overall_r_adl4_versus_r_ad4_ew$horizon <=horizon_parm,]
overall_r_adl4_versus_r_ad4_ic <- overall_r_adl4_versus_r_ad4_ic[overall_r_adl4_versus_r_ad4_ic$horizon <=horizon_parm,]
overall_r_adl4_versus_r_ewc_ic <- overall_r_adl4_versus_r_ewc_ic[overall_r_adl4_versus_r_ewc_ic$horizon <=horizon_parm,]
overall_r_own2_versus_r_adl4   <- overall_r_own2_versus_r_adl4[overall_r_own2_versus_r_adl4$horizon <=horizon_parm,]
overall_r_own2_versus_r_base   <- overall_r_own2_versus_r_base[overall_r_own2_versus_r_base$horizon <=horizon_parm,]
overall_r_own2_versus_r_ow2_ew <- overall_r_own2_versus_r_ow2_ew[overall_r_own2_versus_r_ow2_ew$horizon <=horizon_parm,]
overall_r_own2_versus_r_ow2_ic <- overall_r_own2_versus_r_ow2_ic[overall_r_own2_versus_r_ow2_ic$horizon <=horizon_parm,]
#1. test for MASE
e1  <- unlist(c(overall_r_own2_versus_r_base["q_1"]), use.names=FALSE)   #1 base
e2  <- unlist(c(overall_r_own2_versus_r_base["q_0"]), use.names=FALSE)   #2 own2
e3  <- unlist(c(overall_r_own2_versus_r_adl4["q_1"]), use.names=FALSE)   #3 adl4
e4  <- unlist(c(overall_r_own2_versus_r_ow2_ew["q_1"]), use.names=FALSE) #4 ow2_ew
e5  <- unlist(c(overall_r_own2_versus_r_ow2_ic["q_1"]), use.names=FALSE) #5 ow2_ic
e6  <- unlist(c(overall_r_adl4_versus_r_ad4_ew["q_1"]), use.names=FALSE) #6 ad4_ew
e7  <- unlist(c(overall_r_adl4_versus_r_ad4_ic["q_1"]), use.names=FALSE) #7 ad4_ic
e8  <- unlist(c(overall_r_adl4_versus_r_ewc_ic["q_1"]), use.names=FALSE) #8 ewc_ic
mean(e1)
mean(e2)
mean(e3)
mean(e4)
mean(e5)
mean(e6)
mean(e7)
mean(e8)
t1<- aggregate(overall_r_own2_versus_r_adl4[,13], list(overall_r_own2_versus_r_adl4$category), mean)
t2<- aggregate(overall_r_adl4_versus_r_ad4_ew[,13], list(overall_r_adl4_versus_r_ad4_ew$category), mean)
t3<- aggregate(overall_r_adl4_versus_r_ad4_ic[,13], list(overall_r_adl4_versus_r_ad4_ic$category), mean)
t4<- aggregate(overall_r_adl4_versus_r_ewc_ic[,13], list(overall_r_adl4_versus_r_ewc_ic$category), mean)
library(forecast)
#library(tstools)
library(zoo)
library(PMCMR)
library(NSM3)
rm(list=ls())
setwd("Q:/= IRI data research/Diebold Mariana Test/results data")
#dm.test(e1,e2,h=horizon_parm2,power=1)
#dm test input 'e1' and 'e2' could be the original error, the function will automatically take the absolute value
overall_r_adl4_versus_r_ad4_ew <- read.csv("overall_r_adl4_versus_r_ad4_ew.csv")
overall_r_adl4_versus_r_ad4_ic <- read.csv("overall_r_adl4_versus_r_ad4_ic.csv")
overall_r_adl4_versus_r_ewc_ic <- read.csv("overall_r_adl4_versus_r_ewc_ic.csv")
overall_r_own2_versus_r_adl4   <- read.csv("overall_r_own2_versus_r_adl4.csv")
overall_r_own2_versus_r_base   <- read.csv("overall_r_own2_versus_r_base.csv")
overall_r_own2_versus_r_ow2_ew <- read.csv("overall_r_own2_versus_r_ow2_ew.csv")
overall_r_own2_versus_r_ow2_ic <- read.csv("overall_r_own2_versus_r_ow2_ic.csv")
library(forecast)
#library(tstools)
library(zoo)
library(PMCMR)
library(NSM3)
rm(list=ls())
setwd("Q:/= IRI data research/Manuscript for  SUBMISSION/= rank results and conduct significant test/Results and Diebold and Mariana test/results data")
#dm.test(e1,e2,h=horizon_parm2,power=1)
#dm test input 'e1' and 'e2' could be the original error, the function will automatically take the absolute value
overall_r_adl4_versus_r_ad4_ew <- read.csv("overall_r_adl4_versus_r_ad4_ew.csv")
overall_r_adl4_versus_r_ad4_ic <- read.csv("overall_r_adl4_versus_r_ad4_ic.csv")
overall_r_adl4_versus_r_ewc_ic <- read.csv("overall_r_adl4_versus_r_ewc_ic.csv")
overall_r_own2_versus_r_adl4   <- read.csv("overall_r_own2_versus_r_adl4.csv")
overall_r_own2_versus_r_base   <- read.csv("overall_r_own2_versus_r_base.csv")
overall_r_own2_versus_r_ow2_ew <- read.csv("overall_r_own2_versus_r_ow2_ew.csv")
overall_r_own2_versus_r_ow2_ic <- read.csv("overall_r_own2_versus_r_ow2_ic.csv")
#remove the obs beyond the horizon
horizon_parm <- 8
overall_r_adl4_versus_r_ad4_ew <- overall_r_adl4_versus_r_ad4_ew[overall_r_adl4_versus_r_ad4_ew$horizon <=horizon_parm,]
overall_r_adl4_versus_r_ad4_ic <- overall_r_adl4_versus_r_ad4_ic[overall_r_adl4_versus_r_ad4_ic$horizon <=horizon_parm,]
overall_r_adl4_versus_r_ewc_ic <- overall_r_adl4_versus_r_ewc_ic[overall_r_adl4_versus_r_ewc_ic$horizon <=horizon_parm,]
overall_r_own2_versus_r_adl4   <- overall_r_own2_versus_r_adl4[overall_r_own2_versus_r_adl4$horizon <=horizon_parm,]
overall_r_own2_versus_r_base   <- overall_r_own2_versus_r_base[overall_r_own2_versus_r_base$horizon <=horizon_parm,]
overall_r_own2_versus_r_ow2_ew <- overall_r_own2_versus_r_ow2_ew[overall_r_own2_versus_r_ow2_ew$horizon <=horizon_parm,]
overall_r_own2_versus_r_ow2_ic <- overall_r_own2_versus_r_ow2_ic[overall_r_own2_versus_r_ow2_ic$horizon <=horizon_parm,]
#1. test for MASE
e1  <- unlist(c(overall_r_own2_versus_r_base["q_1"]), use.names=FALSE)   #1 base
e2  <- unlist(c(overall_r_own2_versus_r_base["q_0"]), use.names=FALSE)   #2 own2
e3  <- unlist(c(overall_r_own2_versus_r_adl4["q_1"]), use.names=FALSE)   #3 adl4
e4  <- unlist(c(overall_r_own2_versus_r_ow2_ew["q_1"]), use.names=FALSE) #4 ow2_ew
e5  <- unlist(c(overall_r_own2_versus_r_ow2_ic["q_1"]), use.names=FALSE) #5 ow2_ic
e6  <- unlist(c(overall_r_adl4_versus_r_ad4_ew["q_1"]), use.names=FALSE) #6 ad4_ew
e7  <- unlist(c(overall_r_adl4_versus_r_ad4_ic["q_1"]), use.names=FALSE) #7 ad4_ic
e8  <- unlist(c(overall_r_adl4_versus_r_ewc_ic["q_1"]), use.names=FALSE) #8 ewc_ic
mean(e1)
mean(e2)
mean(e3)
mean(e4)
mean(e5)
mean(e6)
mean(e7)
mean(e8)
t1<- aggregate(overall_r_own2_versus_r_adl4[,13], list(overall_r_own2_versus_r_adl4$category), mean)
t2<- aggregate(overall_r_adl4_versus_r_ad4_ew[,13], list(overall_r_adl4_versus_r_ad4_ew$category), mean)
t3<- aggregate(overall_r_adl4_versus_r_ad4_ic[,13], list(overall_r_adl4_versus_r_ad4_ic$category), mean)
t4<- aggregate(overall_r_adl4_versus_r_ewc_ic[,13], list(overall_r_adl4_versus_r_ewc_ic$category), mean)
